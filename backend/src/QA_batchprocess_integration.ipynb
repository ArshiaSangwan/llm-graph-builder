{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
    "import logging\n",
    "from langchain_community.chat_message_histories import Neo4jChatMessageHistory\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from src.shared.common_fn import load_embedding_model\n",
    "import re\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "#### Defined by Arshia Sangwan 21 June 2024\n",
    "\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL')\n",
    "EMBEDDING_FUNCTION , _ = load_embedding_model(EMBEDDING_MODEL)\n",
    "CHAT_MAX_TOKENS = 1000\n",
    "\n",
    "\n",
    "# RETRIEVAL_QUERY = \"\"\"\n",
    "# WITH node, score, apoc.text.join([ (node)-[:HAS_ENTITY]->(e) | head(labels(e)) + \": \"+ e.id],\", \") as entities\n",
    "# MATCH (node)-[:PART_OF]->(d:Document)\n",
    "# WITH d, apoc.text.join(collect(node.text + \"\\n\" + entities),\"\\n----\\n\") as text, avg(score) as score\n",
    "# RETURN text, score, {source: COALESCE(CASE WHEN d.url CONTAINS \"None\" THEN d.fileName ELSE d.url END, d.fileName)} as metadata\n",
    "# \"\"\"\n",
    "\n",
    "RETRIEVAL_QUERY = \"\"\"\n",
    "WITH node as chunk, score\n",
    "MATCH (chunk)-[:PART_OF]->(d:Document)\n",
    "CALL { WITH chunk\n",
    "MATCH (chunk)-[:HAS_ENTITY]->(e) \n",
    "MATCH path=(e)(()-[rels:!HAS_ENTITY&!PART_OF]-()){0,3}(:!Chunk&!Document) \n",
    "UNWIND rels as r\n",
    "RETURN collect(distinct r) as rels\n",
    "}\n",
    "WITH d, collect(distinct chunk) as chunks, avg(score) as score, apoc.coll.toSet(apoc.coll.flatten(collect(rels))) as rels\n",
    "WITH d, score, \n",
    "[c in chunks | c.text] as texts,  \n",
    "[r in rels | coalesce(apoc.coll.removeAll(labels(startNode(r)),['__Entity__'])[0],\"\") +\":\"+ startNode(r).id + \" \"+ type(r) + \" \" + coalesce(apoc.coll.removeAll(labels(endNode(r)),['__Entity__'])[0],\"\") +\":\" + endNode(r).id] as entities\n",
    "WITH d, score,\n",
    "apoc.text.join(texts,\"\\n----\\n\") +\n",
    "apoc.text.join(entities,\"\\n\")\n",
    "as text, entities\n",
    "RETURN text, score, {source: COALESCE(CASE WHEN d.url CONTAINS \"None\" THEN d.fileName ELSE d.url END, d.fileName), entities:entities} as metadata\n",
    "\"\"\"\n",
    "\n",
    "FINAL_PROMPT = \"\"\"\n",
    "You are an AI-powered question-answering agent tasked with providing accurate and direct responses to user queries. Utilize information from the chat history, current user input, and Relevant Information effectively.\n",
    "\n",
    "Response Requirements:\n",
    "- Deliver concise and direct answers to the user's query without headers unless requested.\n",
    "- Acknowledge and utilize relevant previous interactions based on the chat history summary.\n",
    "- Respond to initial greetings appropriately, but avoid including a greeting in subsequent responses unless the chat is restarted or significantly paused.\n",
    "- For non-general questions, strive to provide answers using chat history and Relevant Information ONLY do not Hallucinate.\n",
    "- Clearly state if an answer is unknown; avoid speculating.\n",
    "\n",
    "Instructions:\n",
    "- Prioritize directly answering the User Input: {question}.\n",
    "- Use the Chat History Summary: {chat_summary} to provide context-aware responses.\n",
    "- Refer to Relevant Information: {vector_result} only if it directly relates to the query.\n",
    "- Cite sources clearly when using Relevant Information in your response [Sources: {sources}] without fail. The Source must be printed only at the last in the format [Source: source1,source2] . Duplicate sources should be removed.\n",
    "Ensure that answers are straightforward and context-aware, focusing on being relevant and concise.\n",
    "\"\"\"\n",
    "\n",
    "def get_llm(model: str,max_tokens=1000) -> Any:\n",
    "    \"\"\"Retrieve the specified language model based on the model name.\"\"\"\n",
    "\n",
    "    model_versions = {\n",
    "        \"OpenAI GPT 3.5\": \"gpt-3.5-turbo-16k\",\n",
    "        \"Gemini Pro\": \"gemini-1.0-pro-001\",\n",
    "        \"Gemini 1.5 Pro\": \"gemini-1.5-pro-preview-0409\",\n",
    "        \"OpenAI GPT 4\": \"gpt-4-0125-preview\",\n",
    "        \"Diffbot\" : \"gpt-4-0125-preview\",\n",
    "        \"OpenAI GPT 4o\":\"gpt-4o\"\n",
    "         }\n",
    "\n",
    "    if model in model_versions:\n",
    "        model_version = model_versions[model]\n",
    "        logging.info(f\"Chat Model: {model}, Model Version: {model_version}\")\n",
    "        \n",
    "        if \"Gemini\" in model:\n",
    "            llm = ChatVertexAI(\n",
    "                model_name=model_version,\n",
    "                convert_system_message_to_human=True,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0,\n",
    "                safety_settings={\n",
    "                    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE, \n",
    "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, \n",
    "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, \n",
    "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            llm = ChatOpenAI(model=model_version, temperature=0,max_tokens=max_tokens)\n",
    "\n",
    "        return llm,model_version\n",
    "\n",
    "    else:\n",
    "        logging.error(f\"Unsupported model: {model}\")\n",
    "        return None,None\n",
    "\n",
    "def vector_embed_results(qa,question):\n",
    "    vector_res={}\n",
    "    try:\n",
    "        result = qa({\"query\": question})\n",
    "        vector_res['result']=result.get(\"result\")\n",
    "\n",
    "        sources = set()\n",
    "        entities = set()\n",
    "        for document in result[\"source_documents\"]:\n",
    "            sources.add(document.metadata[\"source\"])\n",
    "            for entiti in document.metadata[\"entities\"]:\n",
    "                entities.add(entiti)\n",
    "        vector_res['source']=list(sources)\n",
    "        vector_res['entities'] = list(entities)\n",
    "        if len( vector_res['entities']) > 5:\n",
    "            vector_res['entities'] =  vector_res['entities'][:5]\n",
    "            \n",
    "        # list_source_docs=[]\n",
    "        # for i in result[\"source_documents\"]:\n",
    "        #     list_source_docs.append(i.metadata['source'])\n",
    "        #     vector_res['source']=list_source_docs\n",
    "\n",
    "        # result = qa({\"question\":question},return_only_outputs=True)\n",
    "        # vector_res['result'] = result.get(\"answer\")\n",
    "        # vector_res[\"source\"] = result.get(\"sources\")\n",
    "    except Exception as e:\n",
    "      error_message = str(e)\n",
    "      logging.exception(f'Exception in vector embedding in QA component:{error_message}')\n",
    "    #   raise Exception(error_message)\n",
    "    \n",
    "    return vector_res\n",
    "    \n",
    "def save_chat_history(history,user_message,ai_message):\n",
    "    try:\n",
    "        # history = Neo4jChatMessageHistory(\n",
    "        #     graph=graph,\n",
    "        #     session_id=session_id\n",
    "        # )\n",
    "        history.add_user_message(user_message)\n",
    "        history.add_ai_message(ai_message)\n",
    "        logging.info(f'Successfully saved chat history')\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        logging.exception(f'Exception in saving chat history:{error_message}')\n",
    "    \n",
    "def get_chat_history(llm, history):\n",
    "    \"\"\"Retrieves and summarizes the chat history for a given session.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # history = Neo4jChatMessageHistory(\n",
    "        #     graph=graph,\n",
    "        #     session_id=session_id\n",
    "        # )        \n",
    "        chat_history = history.messages\n",
    "        \n",
    "        if not chat_history:\n",
    "            return \"\"\n",
    "\n",
    "        if len(chat_history) > 4:\n",
    "            chat_history = chat_history[-4:]\n",
    "\n",
    "        condense_template = f\"\"\"\n",
    "        Given the following earlier conversation, summarize the chat history. \n",
    "        Make sure to include all relevant information.\n",
    "        Chat History: {chat_history}\n",
    "        \"\"\"\n",
    "        chat_summary = llm.predict(condense_template)\n",
    "        return chat_summary\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Exception in retrieving chat history: {e}\")\n",
    "        return \"\" \n",
    "\n",
    "def clear_chat_history(graph, session_id):\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Clearing chat history for session ID: {session_id}\")\n",
    "        history = Neo4jChatMessageHistory(\n",
    "            graph=graph,\n",
    "            session_id=session_id\n",
    "        )\n",
    "        history.clear()\n",
    "        logging.info(\"Chat history cleared successfully\")\n",
    "\n",
    "        return {\n",
    "            \"session_id\": session_id,\n",
    "            \"message\": \"The chat history is cleared\",\n",
    "            \"user\": \"chatbot\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error occurred while clearing chat history for session ID {session_id}: {e}\")\n",
    "\n",
    "\n",
    "def extract_and_remove_source(message):\n",
    "    pattern = r'\\[Source: ([^\\]]+)\\]'\n",
    "    match = re.search(pattern, message)\n",
    "    if match:\n",
    "        sources_string = match.group(1)\n",
    "        sources = [source.strip().strip(\"'\") for source in sources_string.split(',')]\n",
    "        new_message = re.sub(pattern, '', message).strip()\n",
    "        response = {\n",
    "            \"message\" : new_message,\n",
    "            \"sources\" : sources\n",
    "        }\n",
    "    else:\n",
    "        response = {\n",
    "            \"message\" : message,\n",
    "            \"sources\" : []\n",
    "        }\n",
    "    return response\n",
    "\n",
    "def clear_chat_history(graph,session_id):\n",
    "    history = Neo4jChatMessageHistory(\n",
    "        graph=graph,\n",
    "        session_id=session_id\n",
    "        )\n",
    "    history.clear()\n",
    "    return {\n",
    "            \"session_id\": session_id, \n",
    "            \"message\": \"The chat History is cleared\", \n",
    "            \"user\": \"chatbot\"\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_RAG_batch(graph, model, qa_pairs, session_id):\n",
    "    logging.info(f\"QA_RAG_batch called at {datetime.now()}\")\n",
    "    try:\n",
    "        qa_rag_start_time = time.time()\n",
    "\n",
    "        start_time = time.time()\n",
    "        neo_db = Neo4jVector.from_existing_index(\n",
    "            embedding=EMBEDDING_FUNCTION,\n",
    "            index_name=\"vector\",\n",
    "            retrieval_query=RETRIEVAL_QUERY,\n",
    "            graph=graph\n",
    "        )\n",
    "\n",
    "        history = Neo4jChatMessageHistory(\n",
    "            graph=graph,\n",
    "            session_id=session_id\n",
    "        )\n",
    "        \n",
    "        llm, model_version = get_llm(model=model, max_tokens=CHAT_MAX_TOKENS)\n",
    "\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=neo_db.as_retriever(search_kwargs={'k': 3, \"score_threshold\": 0.7}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "        db_setup_time = time.time() - start_time\n",
    "        logging.info(f\"DB Setup completed in {db_setup_time:.2f} seconds\")\n",
    "\n",
    "        batch_responses = []\n",
    "        for qa_pair in qa_pairs:\n",
    "            question = qa_pair[\"question\"]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            chat_summary = get_chat_history(llm, history)\n",
    "            chat_history_time = time.time() - start_time\n",
    "            logging.info(f\"Chat history summarized in {chat_history_time:.2f} seconds\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            vector_res = vector_embed_results(qa, question)\n",
    "            vector_time = time.time() - start_time\n",
    "            logging.info(f\"Vector response obtained in {vector_time:.2f} seconds\")\n",
    "\n",
    "            formatted_prompt = FINAL_PROMPT.format(\n",
    "                question=question,\n",
    "                chat_summary=chat_summary,\n",
    "                vector_result=vector_res.get('result', ''),\n",
    "                sources=vector_res.get('source', '')\n",
    "            )\n",
    "\n",
    "            start_time = time.time()\n",
    "            response = llm.predict(formatted_prompt)\n",
    "            predict_time = time.time() - start_time\n",
    "            logging.info(f\"Response predicted in {predict_time:.2f} seconds\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            ai_message = response\n",
    "            user_message = question\n",
    "            save_chat_history(history, user_message, ai_message)\n",
    "            chat_history_save = time.time() - start_time\n",
    "            logging.info(f\"Chat History saved in {chat_history_save:.2f} seconds\")\n",
    "\n",
    "            response_data = extract_and_remove_source(response)\n",
    "            message = response_data[\"message\"]\n",
    "            sources = response_data[\"sources\"]\n",
    "\n",
    "            total_call_time = time.time() - qa_rag_start_time\n",
    "            logging.info(f\"Total Response time is {total_call_time:.2f} seconds\")\n",
    "\n",
    "            batch_responses.append({\n",
    "                \"session_id\": session_id,\n",
    "                \"message\": message,\n",
    "                \"info\": {\n",
    "                    \"sources\": sources,\n",
    "                    \"model\": model_version,\n",
    "                    \"entities\": vector_res[\"entities\"]\n",
    "                },\n",
    "                \"user\": \"chatbot\"\n",
    "            })\n",
    "\n",
    "        return batch_responses\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Exception in QA component at {datetime.now()}: {str(e)}\")\n",
    "        error_name = type(e).__name__\n",
    "        return {\n",
    "            \"session_id\": session_id,\n",
    "            \"message\": \"Something went wrong\",\n",
    "            \"info\": {\n",
    "                \"sources\": [],\n",
    "                \"error\": f\"{error_name} :- {str(e)}\"\n",
    "            },\n",
    "            \"user\": \"chatbot\"\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
