{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "from QA_batchprocess import QA_RAG_batch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Neo4j connection\n",
    "uri = os.getenv(\"NEO4J_URI\")\n",
    "user = os.getenv(\"NEO4J_USER\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# Define batch of Q&A pairs\n",
    "qa_pairs_batch = [\n",
    "    [\n",
    "        {\"question\": \"What are the two main tasks BERT is pre-trained on?\",\n",
    "        \"answer\": \"Masked LM (MLM) and Next Sentence Prediction (NSP).\"},\n",
    "        {\"question\": \"What model sizes are reported for BERT, and what are their specifications?\", \"answer\": \"BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"question\": \"How does BERT's architecture facilitate the use of a unified model across diverse NLP tasks?\", \"answer\": \"BERT uses a multi-layer bidirectional Transformer encoder architecture, allowing for minimal task-specific architecture modifications in fine-tuning.\"},\n",
    "        {\"question\": \"Can you describe the modifications LLaMA makes to the transformer architecture for improved performance?\", \"answer\": \" LLaMA incorporates pre-normalization (using RMSNorm), the SwiGLU activation function, and rotary embeddings (RoPE) into its transformer architecture for enhanced training stability and performance.\"},\n",
    "    ]\n",
    "    # Add more batches as needed\n",
    "]\n",
    "\n",
    "# Define the model and session_id\n",
    "model = \"OpenAI GPT 3.5\"\n",
    "session_id = \"001\"\n",
    "\n",
    "# Process each batch of Q&A pairs\n",
    "for qa_pairs in qa_pairs_batch:\n",
    "    with driver.session() as session:\n",
    "        batch_responses = QA_RAG_batch(session, model, qa_pairs, session_id)\n",
    "        for response in batch_responses:\n",
    "            print(response)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
