= LLM Knowledge Graph Builder Backend

== API Reference

=== /connect : 

**Overview:**

Connect with Neo4j database

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
....
**Response :**
....
{
    "status": "Success",
    "message": "Connection Successful"
}
....


=== /sources :

**Overview :**

Create Document node for local file upload

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
file= File uploaded from device, 
model= LLM model 
....
**Response :**
....
{
    "status": "Success",
    "message": "Source Node created successfully",
    "file_source": "local file",
    "file_name": "<Uploaded File Name>"
}
....


=== /url/scan :

**Overview:**

Create Document node for other sources - s3 bucket, gcs bucket, wikipedia and youtube url.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
model= LLM model,
source_url= <s3 bucket url or youtube url> ,
aws_access_key_id= AWS access key,
aws_secret_access_key= AWS secret key,
wiki_query= Wikipedia query sources,
gcs_project_id= GCS project id,
gcs_bucket_name= GCS bucket name,
gcs_bucket_folder= GCS bucket folder,
source_type= s3 bucket/ gcs bucket/ youtube/Wikipedia as source type
gcs_project_id=Form(None),
access_token=Form(None)
....
**Response :**
....
{
    "status": "Success",
    "success_count": 2,
    "failed_count": 0,
    "message": "Source Node created successfully for source type: Wikipedia and source: Albert Einstein,  neo4j",
    "file_name": [
        {
            "fileName": "Albert Einstein",
            "fileSize": 8074,
            "url": "https://en.wikipedia.org/wiki/Albert_Einstein",
            "status": "Success"
        }
    ]
}
....


=== /extract :

**Overview :**

This API is responsible for -

** Reading the content of source provided in the form of langchain Document object from respective langchain loaders 

** Dividing the document into multiple chunks, and make below relations - 
*** PART_OF - relation from Document node to all chunk nodes 
*** FIRST_CHUNK - relation from document node to first chunk node
*** NEXT_CHUNK - relation from a chunk pointing to next chunk of the document.
*** HAS_ENTITY - relation between chunk node and entities extracted from LLM.

** Extracting nodes and relations in the form of GraphDocument from respective LLM.

** Update embedding of chunks and create vector index.

** Update K-Nearest Neighbors graph for similar chunks.


**Implementation :**

** For multiple sources of content - 

*** Local file - User can upload pdf file from their device.

*** s3 bucket - User passes the bucket url and all the pdf files inside folders and subfolders will be listed. 

*** GCS bucket - User passes gcs project id, gcs bucket name and folder name, do google authentication to access all the pdf files under that folder and its subfolders and if folder name is not passed by user, all the pdf files under the bucket and its subfolders will be listed if user have read access of the bucket.

*** Wikipedia - Wikipedia content is rendered (if exist) for comma seprated wikipedia ids or urls passed by user. 

*** Youtube - Youtube video transcript is processed and if no transcript is available then respective error is thrown.

** Langchain's LLMGraphTransformer library is used to get nodes and relations in the form of GraphDocument from LLMs. User and System prompts, LLM chain, graphDocument schema are defined in the library itself.

** SentenceTransformer embeddingds are used by default, also embeddings are made configurable to use either OpenAIEmbeddings or VertexAIEmbeddings.

** Vector index is created in databse on embeddingds created for chunks.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
model= LLM model,
file_name = File uploaded from device
source_url= <s3 bucket url or youtube url> ,
aws_access_key_id= AWS access key,
aws_secret_access_key= AWS secret key,
wiki_query= Wikipedia query sources,
gcs_project_id=GCS project id,
gcs_bucket_name= GCS bucket name,
gcs_bucket_folder= GCS bucket folder,
gcs_blob_filename = GCS file name,
source_type= local file/ s3 bucket/ gcs bucket/ youtube/ Wikipedia as source,
allowedNodes=Node labels passed from settings panel,
allowedRelationship=Relationship labels passed from settings panel,
language=Language in which wikipedia content will be extracted
....
**Response :**
....
{
    "status": "Success",
    "data": {
        "fileName": <PDF File Name/ Wikipedia Query string/ Youtube video title>,
        "nodeCount": <No. Nodes extracted from LLM>,
        "relationshipCount": <No. of relations extracted from LLM>,
        "processingTime": <Total time taken by application to give response>,
        "status": "Completed",
        "model": <LLM Model choosen by User>
    }
}
....

     
=== /sources_list :

**Overview :**

List all sources (Document nodes) present in Neo4j graph database.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
....
**Response :**
....
{
    "status": "Success",
    "data": [
        {
            "fileName": "About Amazon.pdf",
            "fileSize": 163931,
            "errorMessage": "",
            "fileSource": "local file",
            "nodeCount": 62,
            "model": "OpenAI GPT 4",
            "fileType": "pdf",
            "processingTime": 122.71,
            "relationshipCount": 187,
            "status": "Completed",
            "updatedAt": {
                "_DateTime__date": {
                    "_Date__ordinal": 738993,
                    "_Date__year": 2024,
                    "_Date__month": 4,
                    "_Date__day": 17
                },
                "_DateTime__time": {
                    "_Time__ticks": 28640715768000,
                    "_Time__hour": 7,
                    "_Time__minute": 57,
                    "_Time__second": 20,
                    "_Time__nanosecond": 715768000,
                    "_Time__tzinfo": null
                }
            }
        }
    ]
}
....


=== /update_similarity_graph :

**Overview :**

This API is called at the end of processing of whole document to get create k-nearest neighbor relations between similar chunks of document based on KNN_MIN_SCORE which is 0.8 by default.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
....

**Response :**
....
{
    "status":"Success",
    "message":"Updated KNN Graph"
}
....


=== /chat_bot :

**Overview :**

The API responsible for a chatbot system designed to leverage multiple AI models and a Neo4j graph database, providing answers to user queries. It interacts with AI models from OpenAI and Google's Vertex AI and utilizes embedding models to enhance the retrieval of relevant information.

**Components :** 
 
** Embedding Models - Includes OpenAI Embeddings, VertexAI Embeddings, and SentenceTransformer Embeddings to support vector-based query operations.
** AI Models - OpenAI GPT 3.5, GPT 4, Gemini Pro and Gemini 1.5 Pro can be configured for the chatbot backend to generate responses and process natural language.
** Graph Database (Neo4jGraph) - Manages interactions with the Neo4j database, retrieving, and storing conversation histories.
** Response Generation - Utilizes Vector Embeddings from the Neo4j database, chat history, and the knowledge base of the LLM used.

**API Parameters :**
....

uri= Neo4j uri
userName= Neo4j database username
password= Neo4j database password
model= LLM model
question= User query for the chatbot
session_id= Session ID used to maintain the history of chats during the user's connection

....

**Response :**
....
{
  "status": "Success",
  "data": {
    "session_id": "312dd183-4c14-437c-8b2f-8144e5eac646",
    "message": "Hello! How can I assist you today?",
    "sources": [],
    "user": "chatbot"
  }
....

=== /chunk_entities :

**Overview :**


**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
chunk_ids = Chunk ids of document
....

**Response :**
....
{
    
}
....

=== /graph_query :

**Overview :**

This API is used to view graph for a particular file.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
query_type= Neo4j database name
document_names = File name for which user wants to view graph
....

**Response :**
....
{
   
}
....

=== /clear_chat_bot :

**Overview :**

This API is used to clear the chat history which is saved in Neo4j DB.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name,
session_id = User session id for QA chat
....

**Response :**
....
{
    
}
....

=== /connect :

**Overview :**

Neo4j database connection on frontend is done with this API.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
....

**Response :**
....
{
    
}
....

=== /upload :

**Overview :**

When uploading a local file, it gets uploaded in chunks of 1Mb each and user can see the progress of uploading.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name,
file:UploadFile= , 
chunkNumber=, 
totalChunks=, 
originalname=, 
model= LLm model
....

**Response :**
....
{
    
}
....

=== /schema :

**Overview :**

User can set schema for graph generation (i.e. Nodes and relationship labels) in settings panel or get existing db schema through this API. 

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name
....

**Response :**
....
{
    
}
....

=== /delete_document_and_entities :

**Overview :**

Deleteion of nodes and relations for multiple files is done through this API. User can choose multiple documents to be deleted, also user have option to delete only 'Document' and 'Chunk' nodes and keep the entities extracted from that document. 

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name,
filenames= List of files to be deleted,
source_types= Document sources(Wikipedia, youtube, etc.),
deleteEntities= Boolean value to check entities deletion is requested or not
....

**Response :**
....
{
    
}
....

=== /cancelled_job :

**Overview :**

This API is responsible for cancelling an in process job.

**API Parameters :**
....
uri=Neo4j uri, 
userName= Neo4j db username, 
password= Neo4j db password, 
database= Neo4j database name,
filenames= Name of the file whose processing need to be stopped, 
source_types= Source of the file
....

**Response :**
....
{
    
}
....

== Decisions

* Process only 1st page of Wikipedia
* Split document content into chunks of size 200 and overlap of 20
* Configurable elements -
** Number of chunks to combine
** Generate Embedding or not 
** Embedding model
** minimum score for KNN graph
